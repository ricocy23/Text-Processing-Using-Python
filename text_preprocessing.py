# -*- coding: utf-8 -*-
"""Text preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hhIFlVpORxalRJpuah201IbfyAVyu2di

#Data Cleaning
"""

kalimat = '''Indonesia itu indah, terdapat pulau besar seperti Jawa,
Kalimantan, Sumatera, Sulawesi, Bali, Papua'''
lower_case = kalimat.lower()
print(lower_case)

# cleaning non alpha-numeric
import re # impor modul regular expression
txt = 'Hi! @Mukidi, apa kabar?#sapa_Pagi.'
print(re.sub(r'[^\w]',' ',txt))
# atau jika ingin exclude titik dan koma
# re.sub(r'[^.,a-zA-Z0-9 \n\.]','',txt)

txt = 'Hi! @Mukidi, apa kabar? sapa_Pagi.'
print(txt)

# Cleaning hashTags dalam posting media sosial
tweet = '#AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet #d2d'
getHashtags = re.compile(r"#(\w+)")
print("Tags = {0}".format(re.findall(getHashtags, tweet)))

# Extracting atau replacing eMail.
emailPattern = re.compile(r'[\w._%+-]+@[\w\.-]+\.[a-zA-Z]{2,4}')
txt = '''Contact kami di admin@nlpindonesia.org, nlp.indonesia@sci.yahoo.co.id,
atau nlp_nusantara@internet.net'''
print( re.sub(emailPattern, ' ', txt) )# clean email
eMailS = re.findall( emailPattern, txt )
print( 'email yang ditemukan: ', str(eMailS) )

kalimat = "Terdapat 5 jenis ikan dikolam itu,4 lele, gurami, nila, patin, mas"
hasil = re.sub(r"\d+", "", kalimat)
print(hasil)

# Website URLS http(s) .... untuk ftp trivial
urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
txt = 'website reguler expression & my site : https://www.regular-expressions.info/ & https://tau-data.id'
print(re.sub(urlPattern,' ',txt))# clean urls
URLs = re.findall(urlPattern,txt)# get URLs
print('URL yang ditemukan: ',str(URLs))

"""#Tokenizing"""

!pip install nltk

import nltk
nltk.download('punkt')
nltk.download('stopwords')

# impor word_tokenize dari modul nltk
from nltk.tokenize import word_tokenize
kalimat = "Kita sedang belajar Data Mining, materi pre-processing."
tokens = nltk.tokenize.word_tokenize(kalimat)
print(tokens)

#token kata bisa digunakan untuk menghitung kwmunculan kata
from nltk.probability import FreqDist
kalimat = '''Lala sering sekali berbelanja online terutama pada saat pandemi Meurut lala belanja online lebih praktis daripada belanja di pasar.'''
tokens = nltk.tokenize.word_tokenize(kalimat)
kemunculan = nltk.FreqDist(tokens)
print(kemunculan.most_common())

from nltk.tokenize import sent_tokenize
kalimat = '''Kita sedang belajar Data Mining, materi pre-processing.sub materi tokenizing. menyenangkan bukaaaan'''
tokens = nltk.tokenize.sent_tokenize(kalimat)
print(tokens)

"""#Filtering (Stopword Removal)"""

from nltk.corpus import stopwords
kalimat = '''In autumns embrace, leaves gently sway,A symphony of colors in a grand display.'''
tokens = word_tokenize(kalimat)
listStopword = set(stopwords.words('english'))
removed = []
for t in tokens:
    if t not in listStopword:
        removed.append(t)
print(removed)

"""#Stemming"""

!pip install Sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()
kalimat = "Lala sering sekali berbelanja online terutama pada saat pandemi.Menurut lala belanja online dan lebih praktis daripada belanja di pasar."
hasil = stemmer.stem(kalimat)
print(hasil)

"""#Contoh Text Pre-Processing pada dataset"""

import pandas as pd
df = pd.read_csv('ReviewHotel50.csv')
df.head()

import re
def clean_text(df, text_field, new_text_field_name):
    df[new_text_field_name] = df[text_field].str.lower()
    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "",elem))
    # remove numbers
    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))
    return df

df['text_clean'] = df['review'].str.lower()
df['text_clean']
data_clean = clean_text(df, 'review', 'text_clean')
data_clean.head(10)

#------STOPWORDS--------------------
import nltk.corpus
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('indonesian')
data_clean['text_StopWord'] = data_clean['text_clean'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))
data_clean.head()

#--------#TOKONIZE----------------
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize
data_clean['text_tokens'] = data_clean['text_StopWord'].apply(lambda x:word_tokenize(x))
data_clean.head()

#-----------------STEMMING -----------------
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
#import swifter
# create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()
# stemmed
def stemmed_wrapper(term):
    return stemmer.stem(term)
term_dict = {}
hitung=0

for document in data_clean['text_tokens']:
    for term in document:
        if term not in term_dict:
          term_dict[term] = ' '
print(len(term_dict))
print("------------------------")
for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
    hitung+=1
    print(hitung,":",term,":" ,term_dict[term])
    print(term_dict)
    print("------------------------")
# apply stemmed term to dataframe
8
def get_stemmed_term(document):
    return [term_dict[term] for term in document]
#script ini bisa dipisah dari eksekusinya setelah pembacaaan term selesai
data_clean['text_steamindo'] = data_clean['text_tokens'].apply(lambda x:' '.join(get_stemmed_term(x)))
data_clean.head(20)

data_clean.to_csv('hasil_PreProcessing.csv', index= False)

"""#Quiz"""

import re

kalimat = "Terdapat 3 mahasiswa yang berhasil memenangkan lomba, yakni mahasiswa dengan nim 21.11.0570, 21.11.0289, 21.11.0938."
#tampilkan data nim, dari text tersebut di atas, dengan mengenali patternnya.

pattern = r'\d{2}\.\d{2}\.\d{4}'
nims = re.findall(pattern, kalimat)
for nim in nims:
    print("NIM:", nim)